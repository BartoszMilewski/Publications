\documentclass[DaoFP]{subfiles}
\begin{document}
\setcounter{chapter}{2}

\chapter{Sum Types}

\section{Bool}

Now that we know how to compose arrows, it's time to think about composing objects. We've already defined $0$ (the initial object) and $1$ (the terminal object), so how would we go about defining $2$? The obvious answer is that $2$ is $1$ plus $1$, but how do we add objects? 

A $2$ should be an object with two elements: two arrows coming from $1$. Let's call one arrow \hask{True} and the other \hask{False}. Don't confuse these names with the logical interpretations of the initial and the terminal objects. These two are \emph{arrows}. 

\[
 \begin{tikzcd}
 1
 \arrow[dd, bend right, "\text{True}"']
 \arrow[dd, bend left, "\text{False}"]
 \\
 \\
2
 \end{tikzcd}
\]

This simple idea can be immediately expressed in Haskell as the definition of a type, traditionally called \hask{Bool}, after its inventor George Bool (1815-1864).
False

\begin{haskell}
data Bool where
  True  :: () -> Bool
  False :: () -> Bool
\end{haskell}
It corresponds to the same diagram, only with some Haskell renamings:
\[
 \begin{tikzcd}
 \hask{()}
 \arrow[dd, bend right, "\hask{True}"']
 \arrow[dd, bend left, "\hask{False}"]
 \\
 \\
\hask{Bool}
 \end{tikzcd}
\]

As we've seen before, there is a shortcut notation for elements, so here's a more compact version:

\begin{haskell}
data Bool where
  True  :: Bool
  False :: Bool
\end{haskell}

We can now define a term of the type \hask{Bool}, for instance
\begin{haskell}
x :: Bool
x = True
\end{haskell}
The first line declares \hask{x} to be an element of \hask{Bool} (really, a function \hask{()->Bool}), and the second line tells us which one of the two.

The functions \hask{True} and \hask{False} that we used in the definition of \hask{Bool} are called \emph{data constructors}. They can be used to construct specific terms, like in the example above. As a side note, in Haskell, function names start with lower-case letters, except when they are data constructors. 

Our definition of the type \hask{Bool} is still incomplete. We know how to construct a \hask{Bool} term, but we don't know what to do with it. We have to be able to define arrows that go out of \hask{Bool}---the \emph{mappings ou}t of \hask{Bool}. 

The first observation is that, if we have an arrow \hask{h} from \hask{Bool} to some type \hask{A} then we automatically get two arrows $x$ and $y$ from unit to \hask{A}, just by composition.

\[
 \begin{tikzcd}
 \hask{()}
 \arrow[dd, bend right, "\text{True}"']
 \arrow[dd, bend left, "\text{False}"]
  \arrow[ddd, bend right =90, "x"']
 \arrow[ddd, bend left=90, "y"]
\\
 \\
\text{Bool}
\arrow[d, dashed, "h"]
\\
A
 \end{tikzcd}
\]
In other words, every function \hask{Bool->A} produces a pair of elements of \hask{A}.

Given:
\begin{haskell}
h :: Bool -> A
\end{haskell}
we have:
\begin{haskell}
x = h True
y = h False
\end{haskell}
where
\begin{haskell}
x :: A
y :: A
\end{haskell}
Notice the use of the shorthand notation for the application of a function to an element:
\begin{haskell}
h True -- meaning: h . True
\end{haskell}

We are now ready to complete our definition of \hask{Bool} by adding the condition that any function from \hask{Bool} to \hask{A} not only produces but \emph{is equivalent} to a pair of of elements of \hask{A}. In other words, a pair of elements uniquely determines a function from \hask{Bool}. 

What this means is that we can interpret the diagram above in two ways: Given \hask{h}, we can easily get \hask{x} and \hask{y}. But the converse is also true: a pair of elements \hask{x} and \hask{y} uniquely \emph{defines} \hask{h}.

Using the painting analogy, if we assigned a different color to every pair $(x, y)$, we would be able to transfer this color to the corresponding $h$, and each $h$ would be painted differently. 

In Haskell, this definition of \hask{h} is encapsulated in the \hask{if}, \hask{then}, \hask{else} construct. Given
\begin{haskell}
x :: A
y :: A
\end{haskell}
we get
\begin{haskell}
h :: Bool -> A
h b = if b then x else y
\end{haskell}
Here, \hask{b} is a term of type \hask{Bool}. 

In general, a data type is created using \emph{introduction} rules and deconstructed using \emph{elimination} rules. The \hask{Bool} data type has two introduction rules, one using \hask{True} and the other using \hask{False}. The \hask{if}, \hask{then}, \hask{else} defines the elimination rule. 

The fact that, given \hask{h}, we can reconstruct the two terms used to define it, is called the \emph{computation} rule. It tells us how to compute \hask{h}. If the argument evaluates to \hask{True}, then the result is \hask{x}, otherwise it's \hask{y}.

We should never lose sight of the purpose of programming: to decompose complex problems into a series of simpler ones. The definition of \hask{Bool} illustrates this idea. Whenever we have to construct a mapping out of \hask{Bool}, we decompose it into two smaller tasks of constructing elements of the target type. We traded one problem for two simpler ones.

\subsection{Examples}

Let's do a few examples. We haven't defined many types yet, so we'll be limited to \hask{Void}, \hask{()}, and \hask{Bool}. 

We have decided that there can be no functions with \hask{Void} as a target, so we don't expect any functions from \hask{Bool} to \hask{Void}. And indeed, we have zero pairs of elements of \hask{Void}. 

What about functions from \hask{Bool} to \hask{()}? Since \hask{()} is terminal, there can be only one function from \hask{Bool} to it. And, indeed, this function corresponds to the single possible pair of functions from \hask{()} to \hask{()}---both being identities. So far so good.

The interesting case is functions from \hask{Bool} to \hask{Bool}. Let's plug \hask{Bool} in place of \hask{A} in our diagram:
\[
 \begin{tikzcd}
 \hask{()}
 \arrow[dd, bend right, "\text{True}"']
 \arrow[dd, bend left, "\text{False}"]
  \arrow[ddd, bend right =90, "x"']
 \arrow[ddd, bend left=90, "y"]
\\
 \\
\text{Bool}
\arrow[d, dashed, "h"]
\\
\text{Bool}
 \end{tikzcd}
\]
How many pairs of functions from \hask{()} to \hask{Bool} do we have at our disposal? There are only two such functions, \hask{True} and \hask{False}, so we can form four pairs. These are $(True, True)$, $(False, False)$, $(True, False)$, and $(False, True)$. Therefore there can only be four functions from \hask{Bool} to \hask{Bool}. We can write them in Haskell using the  \hask{if}, \hask{then}, \hask{else} construct. For instance, the last one, which we'll call \hask{not} is:
\begin{haskell}
not :: Bool -> Bool
not b = if b then False else True
\end{haskell}

We can also consider the functions from \hask{Bool} to \hask{A} as elements of the object of arrows, or the exponential $A^2$, where $2$ is the \hask{Bool} object. Accoording to our count, we have zero elements in $0^2$, one element in $1^2$, and four elements in $2^2$. This is exactly what we'd expect from high-school algebra.

\section{Enumerations}

What comes after 0, 1, and 2? An object with three data constructors. For instance:
\begin{haskell}
data RGB where
  Red   :: RGB
  Green :: RGB
  Blue  :: RGB
\end{haskell}
If you're tired of repeating the same thing over and over, there is a shorthand for this type of definitions:

\begin{haskell}
data RGB = Red | Green | Blue
\end{haskell}
This introduction pattern allows us to construct terms of the type \hask{RGB}, for instance:
\begin{haskell}
c :: RGB
c = Blue
\end{haskell}
But we need a more general elimination pattern. Just like a function from \hask{Bool} was determined by two elements, a function from \hask{RGB} to \hask{A} is determined by a triple of elements of \hask{A}: \hask{x}, \hask{y}, and \hask{z}. We can write such function using \emph{pattern matching}:
\begin{haskell}
h :: RGB -> A
h Red   = x
h Green = y
h Blue  = z
\end{haskell}
This is just one function whose definition is split into three cases. We could have used this syntax for \hask{Bool} as well:
\begin{haskell}
h :: Bool -> A
h True  = x
h False = y
\end{haskell}
In fact, there is a third way of writing the same thing using the \hask{case} pattern:
\begin{haskell}
h c = case c of
  Red   -> x
  Green -> y
  Blue  -> z
\end{haskell}
or even
\begin{haskell}
h :: Bool -> A
h b = case b of
  True  -> x
  False -> y
\end{haskell}
Use any of these at your convenience when programming.

Obviously, these patterns will also work for types with four, five, and more data constructors. For instance, a decimal digit is one of:
\begin{haskell}
data Digit = One | Two | Three | ... | Nine
\end{haskell}

There is a giant enumeration of Unicode characters called \hask{Char}. Their constructors are given special names: the character itself between two apostrophes, e.g.,
\begin{haskell}
c :: Char
c = 'a'
\end{haskell}
A pattern of ten thousand things would take many years to complete, therefore people came up with the wildcard pattern, the underscore, which matches everything. Because the patterns are matched in order, you make the wildcard pattern the last:
\begin{haskell}
yesno :: Char -> Bool
yesno c = case c of
  'y' -> True
  'Y' -> True
  _   -> False
\end{haskell}

But why should we stop at that? The type \hask{Int} could be thought of as an enumeration of integers in the range between $-2^{29}$ and $2^{29}$ (or more, depending on the implementation). Of course, exhaustive pattern matching on such ranges is out of the question, but the principle holds. 

In practice the types \hask{Char} for Unicode characters, \hask{Int} for fixed-precision integers, \hask{Double} for double-precision floating point numbers, and several others, are built into the language.

These are not infinite types. Their elements can be enumerated, even if it takes ten thousand years.

\section{Sum Types}

The \hask{Bool} type could be seen as the sum $2 = 1 + 1$. But nothing stops us from replacing $1$ with another type, or even replacing each of the $1$s with a different type. We can define a type $A + B$ using two arrows. Let's call them \hask{Left} and \hask{Right}. The defining diagram is:
\[
 \begin{tikzcd}
 A
 \arrow[dr,  "\text{Left}"']
 && B
 \arrow[dl, "\text{Right}"]
 \\
&A + B
 \end{tikzcd}
\]
In Haskell, the type $A + B$ is called \hask{Either A B}. By analogy with \hask{Bool}, we can define it as
\begin{haskell}
data Either A B where
  Left  :: A -> Either A B
  Right :: B -> Either A B
\end{haskell}

By analogy, the mapping out from $A + B$ to some type $C$ is determined by this diagram:
\[
 \begin{tikzcd}
 A
 \arrow[dr,  bend left, "\text{Left}"']
 \arrow[ddr, bend right, "f"']
 && B
 \arrow[dl, bend right, "\text{Right}"]
 \arrow[ddl, bend left, "g"]
 \\
&A + B
\arrow[d, dashed, "h"]
\\
& C
 \end{tikzcd}
\]
Given a function $h$, we get a pair of functions $f$ and $g$ just by composing it with \hask{Left} and \hask{Right}. Conversely, such a pair of functions uniquely determines $h$. 

When we want to translate this diagram to Haskell, we need to select elements of the two types. We can do it by defining the arrows $a$ and $b$ from the terminal object. 
\[
 \begin{tikzcd}
 &1
 \arrow[ld, "a"']
 \arrow[rd, "b"]
 \\
 A
 \arrow[dr,  bend left, "\text{Left}"']
 \arrow[ddr, bend right, "f"']
 && B
 \arrow[dl, bend right, "\text{Right}"]
 \arrow[ddl, bend left, "g"]
 \\
&A + B
\arrow[d, dashed, "h"]
\\
& C
 \end{tikzcd}
\]
Follow the arrows in this diagram to get
\[h \circ \text{Left} \circ a = f \circ a\]
\[h \circ \text{Right} \circ b = g \circ b\]

Haskell syntax repeats these equations almost literally, resulting in this pattern-matching syntax for the definition of \hask{h}:

\begin{haskell}
h :: Either A B -> C
h (Left  a) = f a
h (Right b) = g b
\end{haskell}

Read these equations right to left and you will see the computation rules for sum types. The two functions that were used to define \hask{h} can be recovered by applying \hask{h} to terms constructed using \hask{Left} and \hask{Right}. 

You can also use the \hask{case} syntax to define \hask{h}:
\begin{haskell}
h e = case e of
  Left  a -> f a
  Right b -> g b
\end{haskell}

In logic, the proposition $A + B$ is called the alternative, or \emph{logical or}. You can prove it by providing the proof of $A$ or the proof of $B$. Either one will suffice. 

If you want to prove that $C$ follows from $A+B$, you have to be prepared for two eventualities: either the proof of $A+B$ was obtained by proving $A$ or by proving $B$. In the first case, you need a proof that $C$ follows from $A$. In the second case you need a proof that $C$ follows from $B$. These are the arrows in the definition of $A+B$.

\section{Properties of Sums}

We speak of equality of arrows, but we speak of isomorphism of objects, which in turn is defined by equality of arrows.

When we speak of numbers, we understand that $1 + 0 = 1$. When we speak of objects,  all we can say is that $1 + 0 \cong 1$. 

The easy way to show that two objects are isomorphic is to consider how they interact with other objects. For sums, you look at the outgoing arrows: the mappings out. 

Look at the definition of $1 + 0$ and it's mapping out to any object $A$. It's defined by a pair $(\hask{a}, \hask{absurd})$, where $a$ is an element of $A$. 

\[
 \begin{tikzcd}
 1
 \arrow[dr,  bend left, "\text{Left}"']
 \arrow[ddr, bend right, "a"']
 && 0
 \arrow[dl, bend right, "\text{Right}"]
 \arrow[ddl, bend left, "\text{absurd}"]
 \\
&1 + 0
\arrow[d, dashed, "h"]
\\
& A
 \end{tikzcd}
\]
There is only one \hask{absurd}, but there can be many elements of \hask{A}, and each \hask{a} defines a different \hask{h}. We see that the two objects $1$ and $1 + 0$ have the same set of outgoing arrows to any object $A$. 


\end{document}