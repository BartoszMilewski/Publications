\documentclass[11pt, book]{memoir}

\settrims{0pt}{0pt} % page and stock same size
\settypeblocksize{*}{34.5pc}{*} % {height}{width}{ratio}
\setlrmargins{*}{*}{1} % {spine}{edge}{ratio}
\setulmarginsandblock{1in}{1in}{*} % height of typeblock computed
\setheadfoot{\onelineskip}{2\onelineskip} % {headheight}{footskip}
\setheaderspaces{*}{1.5\onelineskip}{*} % {headdrop}{headsep}{ratio}
\checkandfixthelayout

\chapterstyle{bianchi}
\newcommand{\titlefont}{\normalfont\Huge\bfseries}
\renewcommand{\chaptitlefont}{\titlefont}

\usepackage{amsfonts}
\usepackage{amssymb}  
\usepackage{amsthm} 
\usepackage{amsmath} 
\usepackage{tikz-cd}
\usepackage{float}

\newtheorem{exercise}{Exercise}[section]
\newcommand{\exinline}[1]{(\refstepcounter{exercise}Exercise~\theexercise\label{#1})}

\usepackage{minted}
\newcommand{\hask}[1]{\mintinline{Haskell}{#1}}
\newenvironment{haskell}
  {\VerbatimEnvironment
  	\begin{minted}[escapeinside=??, mathescape=true,frame=single, framesep=5pt, tabsize=1]{Haskell}}
  {\end{minted}}

\begin{document}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}

\title{\huge The Dao of Functional Programming}
\author{\Large Bartosz Milewski }

\date{\vfill (Last updated: \today)}

\maketitle


\frontmatter

Most programming texts, following Brian Kernighan, start with "Hello World!". It's natural to want to get the immediate gratification of making the computer do your bidding and print these famous words. But the real mastery of computer programming goes deeper than that, and rushing into it may only give you a false feeling of power, when in reality you're just parroting the masters. If your ambition is just to learn a useful, well-paid skill then, by all means, feel free to jump right into it. There are tons of books and courses that will teach you to write code in any language of your choice. However, if you really want to get to the essence of programming, you need to be patient and persistent.


\tableofcontents

\mainmatter

\chapter{Clean Slate}

Programming starts with types and functions. You probably have some preconceptions about what types and functions are: get rid of them! They will cloud your mind.

Don't think about how things are implemented in hardware. What computers are is just one of the many models of computation. We shouldn't get attached to it. You can perform computations in your mind, or with pen and paper. The physical substrate is irrelevant to the idea of programming.

\section{Types and Functions}

Paraphrasing Lao Tzu: \emph{The type that can be described is not the eternal type}. In other words, type is a primitive notion. It cannot be defined.

Instead of calling it a \emph{type}, we could as well call it an \emph{object} or a \emph{proposition}. These are the words that are used to describe it in different areas of mathematics (type theory, category theory, and logic, respectively).

There may be more than one type, so we need a way to name them. We could do it by pointing fingers at them, but since we want to effectively communicate with other people, we usually name them. So we'll talk about type $A$, $B$, $C$; or \hask{Int}, \hask{Bool}, \hask{Double}, and so on. These are just names.

A type by itself has no meaning. What makes it special is how it connects to other types. The connections are described by arrows. An arrow has one type as its source and one type as its target. The target could be the same as the source, in which case the arrow loops around.

An arrow between types is called a \emph{function}. An arrow between objects is called a \emph{morphism}. An arrow between propositions is called an \emph{implication}. These are just words that are used to describe arrows in different areas of mathematics. You can use them interchangeably.

A proposition is something that may be true. We may interpret an arrow between two objects $A$ and $B$ as an implication between two propositions. When we say that $A$ \emph{implies} $B$, it means that if $A$ is true then $B$ is also true. In other words, if we can prove $A$, we'll know that $B$ is true.
\pagebreak

There may be more than one arrow between two types, so we need to name them. For instance, here's an arrow called $f$ that goes from type $A$ to type $B$

\[ A \xrightarrow f B \]

One way to interpret this is to say that the function $ f$ takes an argument of type $A$ and produces a result of type $B$. Or that $ f$ is a proof that if $A$ is true then $B$ is also true.

Note: The connection between type theory, lambda calcululus (which is the foundation of programming), logic, and category theory is known as Curry-Howard-Lambek isomorphism.

\section{Yin and Yang}

An object is defined by its connections. An arrow is a proof, a witness, of the fact that two objects are connected. Sometimes there's no proof, the objects are disconnected; sometimes there are many proofs; and sometimes there's a single proof---a unique arrow between two objects.

What does it mean to be \emph{unique}? It means that if you can find two of those, then they must be equal.

An object that has a unique outgoing arrow to every object is called the \emph{initial object}.

Its dual is an object that has a unique incoming arrow from every object. It's called the \emph{terminal object}. 

In mathematics, the initial object is often denoted by $0$ and the terminal object by $1$.

The initial object is the source of everything. As a type it's known as \hask{Void}. It symbolizes the chaos from which everything arises. Since there is an arrow from \hask{Void} to everything, there is also an arrow from \hask{Void} to itself. 

\[
 \begin{tikzcd}
 \hask{Void}
 \arrow[loop]
 \end{tikzcd}
\]

Thus \hask{Void} begets \hask{Void} and everything else.

The terminal object unites everything. As a type it's know as Unit. It symbolizes the ultimate order.

In logic, the terminal object signifies the ultimate truth, symbolized by $T$ or $ \top$. The fact that there's an arrow to it from any object means that $ \top$ is true no matter what your assumptions are. 

Dually, the initial object signifies logical falsehood, contradiction, or a counterfactual. It's written as  False and symbolized by an upside down T, $ \bot$. The fact that there is an arrow from it to any object means that you can prove anything starting from false premises. 

In English, there is special grammatical construct for counterfactual implications. When we say, "If wishes were horses, beggars would ride," we mean that the equality between wishes and horses implies that beggars be able to ride. But we know that the premise is false.

A programming language lets us communicate with each other and with computers. Some languages are easier for the computer to understand, others are closer to the theory. We will use Haskell as a compromise.

In Haskell, the initial object corresponds to the type called \hask{Void}. The name for the terminal type is \hask{()}, a pair of empty parentheses, pronounced Unit. This notation will make sense later.

There are infinitely many types in Haskell, and there is a unique function/arrow from \hask{Void} to each one of them. All these functions are known under the same name: \hask{absurd}.

\begin{center}
\begin{tabular} {|c | c | c|}
\hline
Programming & Category theory & Logic \\
\hline
type & object & proposition \\
function & morphism (arrow) & implication \\
\hask{Void} & initial object, $0$ & False $\bot$ \\
\hask{()} & terminal object, $1$ & True $\top$ \\
\hline

\end{tabular}
\end{center}

\section{Elements}

An object has no parts but it may have structure. The structure is defined by the arrows pointing at the object. We can \emph{probe} the object with arrows.

In programming and in logic we want our initial object to have no structure. So we'll assume that it has no incoming arrows (other than the one that's looping back from it). Therefore \hask{Void} has no structure. 

The terminal object has the simplest structure. There is only one incoming arrow from any object to it: there is only one way of probing it from any direction. In this respect, the terminal object behaves like an indivisible point. Its only property is that it exists, and the arrow from any other object proves it. 

Because the terminal object is so simple, we can use it to probe other, more complex objects. 

If there is more than one arrow coming from the terminal object to some object A, it means that A has some structure: there is more than one way of looking at it. Since the terminal object behaves like a point, we can visualize each arrow from it as picking a different point or element of its target. 

In category theory we say that $ x$ is a \emph{global element} of $ A$ if it's an arrow

\[ 1 \xrightarrow x A \]

We'll often simply call it an element (omitting "global").

In logic, such $ x$ is called the proof of $ A$, since it corresponds to the implication $ \top \to A$ (if \textbf{True} is true then \textbf{A} is true). Notice that there may be many different proofs of $A$.

In type theory, $ x \colon A$ means that $ x$ is of type $ A$.

In Haskell, we use the double-colon notation instead, but it means the same:

\begin{haskell}
x :: A
\end{haskell}
We say that \hask{x} is a term of type \hask{A}, but we can also look at it as a function \hask{() -> A}, a global element of \hask{A}. 

Since we have assumed that there be no arrows from any other object to \hask{Void}, there is no arrow from the terminal object to it. Therefore \hask{Void} has no elements. This is why we think of \hask{Void} as empty.

The terminal object has just one element, since there is a unique arrow coming from it to itself, $ 1 \to 1$. This is why we sometimes call it a singleton. 

Note: In category theory there is no prohibition against the initial object having incoming arrows from other objects. However, in cartesian closed categories that we're studying here, this is not allowed.

\section{The Object of Arrows}

We describe an object by looking at its arrows. Can we describe an arrow in a similar way? Could an arrow from $A$ to $B$ be represented as an element of some special \emph{object of arrows}? After all, in programming we talk about the \emph{type} of functions from \hask{A} to \hask{B}. In Haskell we write:
\begin{haskell}
f :: A -> B
\end{haskell}
meaning that \hask{f} is of the type ``function from A to B''. Here, \hask{A->B} is just the name we are giving to this type. To fully define this type, we would have to describe its relation to other objects, in particular to A and B. We don't have the tools to do that yet, but we'll get there. 

For now, let's keep in mind the following distinction: On the one hand we have an arrow which connects two objects \hask{A} and \hask{B}. On the other hand we have an element of the \emph{object of arrows} from  \hask{A} to \hask{B}. This element is itself defined as an arrow from the terminal object \hask{()} to the object we call \hask{A->B}. 

The notation we use in programming tends to blur this distinction. This is why in category theory we call the object of arrows an \emph{exponential} and write it as $ B^A$ (the source is in the exponent). So the statement:
\begin{haskell}
f :: A -> B
\end{haskell}
is equivalent to

\[ 1 \xrightarrow f B^A\]

In logic, an arrow $ A \to B$ is an implication: it states the fact that "if A then B." An exponential object $ B^A$ is the corresponding proposition. It could be true or it could be false, we don't know. You have to prove it. Such a proof is an element of $ B^A$. 

Show me an element of $ B^A$ and I'll know that $ B$ follows from $ A$.

Consider again the statement, "If wishes were horses, beggars would ride"---this time as an object. It's not an empty object, because you can point at a proof of it---something along the lines: "A person who has a horse rides it. Beggars have wishes. Since wishes are horses, beggars have horses. Therefore beggars ride." But, even though you have a proof of this statement, it's of no use to you, because you can never prove its premise: "wish = horse". 

\chapter{Composition}

\section{Composition}

Programming is about composition. Paraphrasing Wittgenstein, we could say: ``Of that which cannot be decomposed one should not speak.'' This is not a prohibition, it's a statement of fact. The process of studying, understanding, and describing is the same as the process of decomposing, and our language reflects this. 

The reason we have built the vocabulary of objects and arrows is precisely to express the idea of composition.  

Given an arrow $f$ from $A$ to $B$ and an arrow $g$ from $B$ to $C$, their composition is an arrow that goes directly from $A$ to $C$. In other words, if there are two arrows, the target of one being the same as the source of the other, we can always compose them to get a third arrow.

\[
 \begin{tikzcd}
 A
 \arrow[rr, bend left, "h"]
 \arrow[r, "f"']
 & B
 \arrow[r, "g"']
& C
 \end{tikzcd}
\]

In math we denote composition using a little circle
\[h = g \circ f\]
We read this: ``$h$ is equal to $g$ after $f$.'' The order of composition might seem backward, but this is because we think of functions as taking arguments on the right.
In Haskell we replaced the circle with a dot:
\begin{haskell}
h = g . f
\end{haskell}
This is every program in a nutshell. In order to accomplish \hask{h}, we decompose it into simpler problems, \hask{f} and \hask{g}. These, in turn, can be decomposed further, and so on.

Now suppose that we were able to decompose $g$ itself into $j \circ k$. We have
\[h = (j \circ k) \circ f\]
We want this decomposition to be the same as
\[h = j \circ (k \circ f)\]
We want to be able to say that we have decomposed $h$ into three simpler problems
\[h =  j \circ k \circ f\]
and not have to keep track which decomposition came first. This is called \emph{associativity} of composition, and we will assume it from now on.

Composition is the source of two mappings of arrows called pre-composition and post-composition. 

When an arrow $f$ is post-composed after an arrow $h$, it produces the arrow $f \circ h$. Of course, $f$ can be post-composed only after arrows whose target is the source of $f$. Post-composition by $f$ is written as $(f \circ -)$, leaving a hole for $h$. As Lao Tzu would say, "Usefulness of post-composition comes from what is not there."

Thus an arrow $f \colon A \to B$ induces a mapping of arrows $(f \circ -)$ that maps arrows which are probing $A$ to arrows that are probing $B$. 

\[
 \begin{tikzcd}
 \node (a1) at (0, 2) {};
 \node (a2) at (-0.5, 2) {};
 \node (a3) at (0.5, 2) {};
 \node (aa) at (0.5, 1) {};
 \node(a) at (0, 0) {A};
 \draw[->, red] (a1) -- (a);
 \draw[->] (a2) -- (a);
 \draw[->, blue] (a3) -- (a);
 \node (b1) at (3+0, 2) {};
 \node (b2) at (3-0.5, 2) {};
 \node (b3) at (3+0.5, 2) {};
 \node (bb) at (2.5, 1) {};
 \node(b) at (3, 0) {B};
 \draw[->, red] (b1) -- (b);
 \draw[->] (b2) -- (b);
 \draw[->, blue] (b3) -- (b);
 \draw[->] (a) -- node[below]{f} (b);
 \draw[->, dashed] (aa) -- node[above]{(f \circ -)} (bb);
  \end{tikzcd}
\]
Since objects have no internal structure, when we say that $f$ transforms $A$ to $B$, this is exactly what we mean. 

Post-composition lets us shift focus from one object to another.

Dually, you can pre-compose $f$, or apply $(- \circ f)$ to arrows originating in $B$ and map them to arrows originating in $A$. 

\[
 \begin{tikzcd}
 \node (a1) at (0, 2) {};
 \node (a2) at (-0.5, 2) {};
 \node (a3) at (0.5, 2) {};
 \node (aa) at (0.5, 1) {};
 \node(a) at (0, 0) {A};
 \draw[<-, red] (a1) -- (a);
 \draw[<-] (a2) -- (a);
 \draw[<-, blue] (a3) -- (a);
 \node (b1) at (3+0, 2) {};
 \node (b2) at (3-0.5, 2) {};
 \node (b3) at (3+0.5, 2) {};
 \node (bb) at (2.5, 1) {};
 \node(b) at (3, 0) {B};
 \draw[<-, red] (b1) -- (b);
 \draw[<-] (b2) -- (b);
 \draw[<-, blue] (b3) -- (b);
 \draw[->] (a) -- node[below]{f} (b);
 \draw[->, dashed] (bb) -- node[above]{(- \circ f)} (aa);
  \end{tikzcd}
\]

Pre-composition let us shift the perspective from one observer to another. Notice that the outgoing arrows are mapped in the direction opposite to the shifting arrow $f$.

Do the following exercises to convince yourself that shifts in focus and perspective are composable.
\begin{exercise}\label{ex-yoneda-composition}
Suppose that you have two arrows, $f \colon A \to B$ and $g \colon B \to C$. Their composition induces a mapping of arrows $((g \circ f) \circ -)$. Show that the result is the same if you first apply $(f \circ -)$ and follow it by $(g \circ -)$. Hint: Pick an arbitrary object $X$ and an arrow $h \colon X \to A$ and see if you get the same result. 
\end{exercise}

\begin{exercise}
Convince yourself that the composition from the previous exercise is associative. Hint: Start with three composable arrows.
\end{exercise}

\begin{exercise}
Show that pre-composition $(- \circ f)$ is composable, but the order of composition is reversed.
\end{exercise}

\section{Function application}

We are ready to write our first program. There is a saying: "A journey of a thousand miles begins with a single step." Our journey is from $1$ to $B$. The single step is an arrow from the terminal object $1$ to $A$. It's an element of $A$. We can write it as
\[1 \xrightarrow x A \]
The rest of the journey is the arrow 
\[A \xrightarrow f B\]
These two arrows are composable (they share the object $A$) and their composition is the arrow $y$ from $1$ to $B$. In other words, $y$ is an \emph{element} of $B$

\[
 \begin{tikzcd}
 1
 \arrow[rr, bend left, "y"]
 \arrow[r, "x"']
 & A
 \arrow[r, "f"']
& B
 \end{tikzcd}
\]
We can write it as
\[y = f \circ x \]

We used $f$ to map an \emph{element} of $A$ to an \emph{element} of $B$. Since this is something we do quite often, we call it the \emph{application} of a function $f$ to $x$, and use the shorthand notation
\[y = f x \]
Let's translate it to Haskell. We start with an element of $A$ (a shorthand for \hask{()->A})
\begin{haskell}
x :: A
\end{haskell}
We declare the function $f$ as an element of the ``object of arrows'' from $A$ to $B$
\begin{haskell}
f :: A -> B
\end{haskell}
The result is an element of $B$
\begin{haskell}
y :: B
\end{haskell}
and it is defined as
\begin{haskell}
y = f x
\end{haskell}
We call this the application of a function to an argument, but we expressed it purely in terms of function composition. (Note: In other programming languages function application requires the use of parentheses, e.g., \hask{y = f(x)}.)

\section{Identity}

You may think of arrows as representing change: object $A$ becomes object $B$. An arrow that loops back represents a change in an object itself. But change has its dual: lack of change, inaction or, as Lao Tze would say \emph{wu wei}. 

Every object has a special arrow called the identity, which leaves the object unchanged. It means that, when you compose this arrow with any other arrow, either incoming or outgoing, you get that arrow back. Identity arrow does nothing to an arrow. 

An identity arrow on the object $A$ is called $id_A$. So if we have an arrow $f \colon A \to B$, we can compose it with identities on either side

\[id_B \circ f = f = f \circ id_A \]
or, pictorially:
\[
 \begin{tikzcd}
 A
 \arrow[loop, "id_A"']
 \arrow[r, "f"]
 & B
 \arrow[loop, "id_B"']
 \end{tikzcd}
\]

We can easily check what an identity does to elements. Let's take an element $x \colon 1 \to A$ and compose it with $id_A$. The result is:
\[id_A \circ x = x\]
which means that identity leaves elements unchanged.

In Haskell, we use the same name \hask{id} for all identity functions (we omit the object/type). The above equation, which specifies the action of $id$ on elements, translates directly to
\begin{haskell}
id x = x
\end{haskell}
and it becomes the definition of the function \hask{id}. 

We've seen before that both the initial object and the terminal object have unique arrows circling back to them. Now we are saying that every object has an identity arrow circling back to it. Remember what we said about uniqueness: If you can find two of those, then they must be equal. We must conclude that these unique looping arrows we talked about must be the identity arrows. We can now label these diagrams:

\[
 \begin{tikzcd}
 \hask{Void}
 \arrow[loop, "id"']
 \end{tikzcd}
 \begin{tikzcd}
 \hask{()}
 \arrow[loop, "id"']
 \end{tikzcd}
\]

In logic, identity arrow translates to a tautology. It's a trivial proof that, "if $A$ is true then $A$ is true."
\begin{exercise}\label{ex-yoneda-identity}
What does $(id_A \circ -)$ do to arrows terminating in $A$? What does $(- \circ id_A)$ do to arrows originating from $A$?
\end{exercise}


\section{Isomorphisms}

If identity does nothing then why do we care about it? Imagine going on a trip, composing a few arrows, and finding yourself back at the starting point. The question is: Have you done anything, or have you wasted your time? The only way to answer this question is to compare your path with the identity arrow. 

Some round trips bring change, others don't.

The simplest round trip is a composition of two arrows going in opposite directions. 
\[
 \begin{tikzcd}
 A
 \arrow[r, bend left, "f"]
 & B
 \arrow[l, bend left, "g"]
 \end{tikzcd}
\]

There are two possible round trips. One is $g \circ f$, which goes from $A$ to $A$. The other is $f \circ g$ , which goes from $B$ to $B$. If both of them can be replaced with identities, than we say that $g$ is the inverse of $f$
\[ g \circ f = id_A\]
\[f \circ g = id_B\]
and we write it as $g = f^{-1}$. The arrow $ f^{-1}$ undoes the work of the arrow $f$. 

Such a pair of arrows is called an \emph{isomorphism}.

What does the existence of an isomorphisms tell us about the two objects they connect? 

We said that objects are described by their interactions with other objects. So let's consider what these objects look like from the perspective of an observer object $X$. Take an arrow $h$ coming from $X$ to $A$.

\[
 \begin{tikzcd}
 & X
 \arrow[ld, "h"']
 \\
 A
 \arrow[rr, "f"]
  && B
 \arrow[ll, bend left,  "f^{-1}"]
 \end{tikzcd}
\]
There is a corresponding arrow coming from $X$ to $B$. It's just the composition of $f \circ h$, or the action of $(f \circ -)$ on $h$.
\[
 \begin{tikzcd}
 & X
 \arrow[ld, "h"']
 \arrow[rd, "f \circ h"]
 \\
 A
 \arrow[rr, "f"]
  && B
 \arrow[ll, bend left,  "f^{-1}"]
 \end{tikzcd}
\]
Similarly, for any arrow probing $B$ there is a corresponding arrow probing $A$. It is given by the action of  $(f^{-1} \circ -)$. 

We can move focus back and forth between $A$ and $B$ using $(f \circ -)$ and $(f^{-1} \circ -)$.

We can combine these two mappings (see exercise \ref{ex-yoneda-composition}) to form a round trip. The result is the same as if we applied the composite $((f^{-1} \circ f) \circ -)$. But this is equal to $(id_A \circ  -)$ which, as we know from exercise \ref{ex-yoneda-identity}, leaves the arrows unchanged.

Similarly, the round trip induced by $f \circ f^{-1}$ leaves the arrows $X \to B$ unchanged. 

Imagine each arrow sending a message to its buddy, as determined by $f$ or $f^{-1}$. Each arrow would then find out that it received exactly one message, and it's a message from the same buddy. No arrow would be left behind, and no arrow would receive more than one message. Mathematicians call this kind of buddy system a \emph{bijection}.

Therefore, arrow by arrow, the two objects $A$ and $B$ look exactly the same from the perspective of $X$. 

Arrow-wise, there is no difference between the two objects. 

In particular, if you replace $X$ with the terminal object $1$, you'll see that the two objects have the same elements. For every element $x \colon 1 \to A$ there is a corresponding element $y \colon 1 \to B$, namely $y = f \circ x$, and vice versa.

Such indistinguishable objects are called \emph{isomorphic}, meaning ``the same shape.'' You've seen one, you've seen them all. We write this isomorphism as:

\[A \cong B\]

In classical logic, if B follows from A and A follows from B then A and B are logically equivalent. We often say that B is true "if and only if" A is true. However, unlike previous parallels between logic and type theory, this one is not as straightforward and, in fact, led to the development of a new branch of fundamental mathematics, the homotopy type theory, or HoTT for short.

\begin{exercise}
Make an argument that there is also a bijection between arrows \emph{outgoing} from two isomorphic objects. Draw the corresponding diagrams.
\end{exercise}


\begin{exercise}
Show that every object is isomorphic to itself
\end{exercise}

\begin{exercise}
If there are two terminal objects, show that they are isomorphic
\end{exercise}

\begin{exercise}
Show that the isomorphism from the previous exercise is unique.
\end{exercise}

\section{Naturality}

When two objects are isomorphic, we can switch focus from one to another using post-composition, with either $(f \circ -)$ or $(f^{-1} \circ -)$. To switch between different observers, we use pre-composition. 



If two probing objects, $X$ and $Y$ are connected by an arrow $g \colon Y \to X$, their views of the isomorphic objects are not unrelated. For instance, an arrow $h$ probing $A$ from $X$ is related to the arrow $h\circ g$ probing the same object $A$ from $Y$. 

\[
 \begin{tikzcd}
 X
 \arrow[d, "h"']
 && Y
 \arrow[ll, "g"']
  \arrow[dll, "h \circ g"']
 \\
 A
 \arrow[rr, "f"]
  && B
 \arrow[ll, bend left,  "f^{-1}"]
 \end{tikzcd}
\]
Similarly, an arrow $h'$ probing $B$ from $X$ corresponds to the arrow $h' \circ g$ probing it from $Y$.

\[
 \begin{tikzcd}
 X
 \arrow[drr, "h'"]
 && Y
 \arrow[ll, "g"']
  \arrow[d, "h' \circ g"]
 \\
 A
 \arrow[rr, "f"]
  && B
 \arrow[ll, bend left,  "f^{-1}"]
 \end{tikzcd}
\]

If $h$ and $h'$ are "buddies" in the $X$ frame of reference, then $g \circ h$ and $g \circ h'$ should be "buddies" in the $Y$ frame of reference. And indeed they are! 

The "buddy" condition is $h' = f \circ h$. When we pre-compose it with $g$ we get
\[h'  \circ g = f \circ h \circ g\]
which says that $h' \circ g$ is the buddy of $h \circ g$. 

For isomorphic objects, the "buddy" relation between arrows is preserved when changing the reference frame to another object. Once buddies, buddies forever!

Both, the "buddy" relation and the change of reference frame can be described using "composition with a hole." We started with $h$, found its buddy $h'$ using $(f \circ -)$, and then switched to the $Y$ frame by applying $(- \circ g)$. The result was the same if we first changed the frame using $(- \circ g)$ and then found its buddy in the $Y$ frame using $(f \circ -)$. If we use the same symbol $\circ$ for composition of compositions, we can write it as:

\[(- \circ g) \circ (f \circ -) = (f \circ -) \circ (- \circ g)\]

In other words, it doesn't matter if you first post-compose with $f$ and then pre-compose with $g$, or first pre-compose with $g$ and then post-compose with $f$. 

\section{The Yoneda Connection}

We have shown that if two objects are isomorphic, then they have the same sets of incoming (or, alternatively, outgoing) arrows. One of the most important and useful results in category theory is the converse of this statement. 

Two objects are isomorphic if you can show that the sets of incoming arrows from all object are the same. 

Since the essence of objects is in their arrows, when you want to compare objects, compare their arrows.

This is a special, but very important, application of the Yoneda lemma.


\chapter{Sum Types}

\section{Bool}

Now that we know how to compose arrows, it's time to think about composing objects. We've already defined $0$ (the initial object) and $1$ (the terminal object), so how would we go about defining $2$? The obvious answer is that $2$ is $1$ plus $1$, but how do we add objects? 

A $2$ should be an object with two elements: two arrows coming from $1$. Let's call one arrow \hask{True} and the other \hask{False}. Don't confuse these names with the logical interpretations of the initial and the terminal objects. These two are \emph{arrows}. 

\[
 \begin{tikzcd}
 1
 \arrow[dd, bend right, "\text{True}"']
 \arrow[dd, bend left, "\text{False}"]
 \\
 \\
2
 \end{tikzcd}
\]

This simple idea can be immediately expressed in Haskell as the definition of a type, traditionally called \hask{Bool}, after its inventor George Bool (1815-1864).
False

\begin{haskell}
data Bool where
  True  :: () -> Bool
  False :: () -> Bool
\end{haskell}
It corresponds to the same diagram, only with some Haskell renamings:
\[
 \begin{tikzcd}
 \hask{()}
 \arrow[dd, bend right, "\hask{True}"']
 \arrow[dd, bend left, "\hask{False}"]
 \\
 \\
\hask{Bool}
 \end{tikzcd}
\]

As we've seen before, there is a shortcut notation for elements, so here's a more compact version:

\begin{haskell}
data Bool where
  True  :: Bool
  False :: Bool
\end{haskell}

We can now define a term of the type \hask{Bool}, for instance
\begin{haskell}
x :: Bool
x = True
\end{haskell}
The first line declares \hask{x} to be an element of \hask{Bool} (really, a function \hask{()->Bool}), and the second line tells us which one of the two.

The functions \hask{True} and \hask{False} that we used in the definition of \hask{Bool} are called \emph{data constructors}. They can be used to construct specific terms, like in the example above. As a side note, in Haskell, function names start with lower-case letters, except when they are data constructors. 

Our definition of the type \hask{Bool} is still incomplete. We know how to construct a \hask{Bool} term, but we don't know what to do with it. We have to be able to define arrows that go out of \hask{Bool}---the \emph{mappings ou}t of \hask{Bool}. 

The first observation is that, if we have an arrow \hask{h} from \hask{Bool} to some type \hask{A} then we automatically get two arrows $x$ and $y$ from unit to \hask{A}, just by composition.

\[
 \begin{tikzcd}
 \hask{()}
 \arrow[dd, bend right, "\text{True}"']
 \arrow[dd, bend left, "\text{False}"]
  \arrow[ddd, bend right =90, "x"']
 \arrow[ddd, bend left=90, "y"]
\\
 \\
\text{Bool}
\arrow[d, dashed, "h"]
\\
A
 \end{tikzcd}
\]
In other words, every function \hask{Bool->A} produces a pair of elements of \hask{A}.

Given:
\begin{haskell}
h :: Bool -> A
\end{haskell}
we have:
\begin{haskell}
x = h True
y = h False
\end{haskell}
where
\begin{haskell}
x :: A
y :: A
\end{haskell}
Notice the use of the shorthand notation for the application of a function to an element:
\begin{haskell}
h True -- meaning: h . True
\end{haskell}

We are now ready to complete our definition of \hask{Bool} by adding the condition that any function from \hask{Bool} to \hask{A} not only produces but \emph{is equivalent} to a pair of of elements of \hask{A}. In other words, a pair of elements uniquely determines a function from \hask{Bool}. 

What this means is that we can interpret the diagram above in two ways: Given \hask{h}, we can easily get \hask{x} and \hask{y}. But the converse is also true: a pair of elements \hask{x} and \hask{y} uniquely \emph{defines} \hask{h}.

Using the painting analogy, if we assigned a different color to every pair $(x, y)$, we would be able to transfer this color to the corresponding $h$, and each $h$ would be painted differently. 

In Haskell, this definition of \hask{h} is encapsulated in the \hask{if}, \hask{then}, \hask{else} construct. Given
\begin{haskell}
x :: A
y :: A
\end{haskell}
we get
\begin{haskell}
h :: Bool -> A
h b = if b then x else y
\end{haskell}
Here, \hask{b} is a term of type \hask{Bool}. 

In general, a data type is created using \emph{introduction} rules and deconstructed using \emph{elimination} rules. The \hask{Bool} data type has two introduction rules, one using \hask{True} and the other using \hask{False}. The \hask{if}, \hask{then}, \hask{else} defines the elimination rule. 

The fact that, given \hask{h}, we can reconstruct the two terms used to define it, is called the \emph{computation} rule. It tells us how to compute \hask{h}. If the argument evaluates to \hask{True}, then the result is \hask{x}, otherwise it's \hask{y}.

We should never lose sight of the purpose of programming: to decompose complex problems into a series of simpler ones. The definition of \hask{Bool} illustrates this idea. Whenever we have to construct a mapping out of \hask{Bool}, we decompose it into two smaller tasks of constructing elements of the target type. We traded one problem for two simpler ones.

\subsection{Examples}

Let's do a few examples. We haven't defined many types yet, so we'll be limited to \hask{Void}, \hask{()}, and \hask{Bool}. 

We have decided that there can be no functions with \hask{Void} as a target, so we don't expect any functions from \hask{Bool} to \hask{Void}. And indeed, we have zero pairs of elements of \hask{Void}. 

What about functions from \hask{Bool} to \hask{()}? Since \hask{()} is terminal, there can be only one function from \hask{Bool} to it. And, indeed, this function corresponds to the single possible pair of functions from \hask{()} to \hask{()}---both being identities. So far so good.

The interesting case is functions from \hask{Bool} to \hask{Bool}. Let's plug \hask{Bool} in place of \hask{A} in our diagram:
\[
 \begin{tikzcd}
 \hask{()}
 \arrow[dd, bend right, "\text{True}"']
 \arrow[dd, bend left, "\text{False}"]
  \arrow[ddd, bend right =90, "x"']
 \arrow[ddd, bend left=90, "y"]
\\
 \\
\text{Bool}
\arrow[d, dashed, "h"]
\\
\text{Bool}
 \end{tikzcd}
\]
How many pairs of functions from \hask{()} to \hask{Bool} do we have at our disposal? There are only two such functions, \hask{True} and \hask{False}, so we can form four pairs. These are $(True, True)$, $(False, False)$, $(True, False)$, and $(False, True)$. Therefore there can only be four functions from \hask{Bool} to \hask{Bool}. We can write them in Haskell using the  \hask{if}, \hask{then}, \hask{else} construct. For instance, the last one, which we'll call \hask{not} is:
\begin{haskell}
not :: Bool -> Bool
not b = if b then False else True
\end{haskell}

We can also consider the functions from \hask{Bool} to \hask{A} as elements of the object of arrows, or the exponential $A^2$, where $2$ is the \hask{Bool} object. Accoording to our count, we have zero elements in $0^2$, one element in $1^2$, and four elements in $2^2$. This is exactly what we'd expect from high-school algebra.

\section{Enumerations}

What comes after 0, 1, and 2? An object with three data constructors. For instance:
\begin{haskell}
data RGB where
  Red   :: RGB
  Green :: RGB
  Blue  :: RGB
\end{haskell}
If you're tired of repeating the same thing over and over, there is a shorthand for this type of definitions:

\begin{haskell}
data RGB = Red | Green | Blue
\end{haskell}
This introduction pattern allows us to construct terms of the type \hask{RGB}, for instance:
\begin{haskell}
c :: RGB
c = Blue
\end{haskell}
But we need a more general elimination pattern. Just like a function from \hask{Bool} was determined by two elements, a function from \hask{RGB} to \hask{A} is determined by a triple of elements of \hask{A}: \hask{x}, \hask{y}, and \hask{z}. We can write such function using \emph{pattern matching}:
\begin{haskell}
h :: RGB -> A
h Red   = x
h Green = y
h Blue  = z
\end{haskell}
This is just one function whose definition is split into three cases. We could have used this syntax for \hask{Bool} as well:
\begin{haskell}
h :: Bool -> A
h True  = x
h False = y
\end{haskell}
In fact, there is a third way of writing the same thing using the \hask{case} pattern:
\begin{haskell}
h c = case c of
  Red   -> x
  Green -> y
  Blue  -> z
\end{haskell}
or even
\begin{haskell}
h :: Bool -> A
h b = case b of
  True  -> x
  False -> y
\end{haskell}
Use any of these at your convenience when programming.

Obviously, these patterns will also work for types with four, five, and more data constructors. For instance, a decimal digit is one of:
\begin{haskell}
data Digit = One | Two | Three | ... | Nine
\end{haskell}

There is a giant enumeration of Unicode characters called \hask{Char}. Their constructors are given special names: the character itself between two apostrophes, e.g.,
\begin{haskell}
c :: Char
c = 'a'
\end{haskell}
A pattern of ten thousand things would take many years to complete, therefore people came up with the wildcard pattern, the underscore, which matches everything. Because the patterns are matched in order, you make the wildcard pattern the last:
\begin{haskell}
yesno :: Char -> Bool
yesno c = case c of
  'y' -> True
  'Y' -> True
  _   -> False
\end{haskell}

But why should we stop at that? The type \hask{Int} could be thought of as an enumeration of integers in the range between $-2^{29}$ and $2^{29}$ (or more, depending on the implementation). Of course, exhaustive pattern matching on such ranges is out of the question, but the principle holds. 

In practice the types \hask{Char} for Unicode characters, \hask{Int} for fixed-precision integers, \hask{Double} for double-precision floating point numbers, and several others, are built into the language.

These are not infinite types. Their elements can be enumerated, even if it takes ten thousand years.

\section{Sum Types}

The \hask{Bool} type could be seen as the sum $2 = 1 + 1$. But nothing stops us from replacing $1$ with another type, or even replacing each of the $1$s with a different type. We can define a type $A + B$ using two arrows. Let's call them \hask{Left} and \hask{Right}. The defining diagram is:
\[
 \begin{tikzcd}
 A
 \arrow[dr,  "\text{Left}"']
 && B
 \arrow[dl, "\text{Right}"]
 \\
&A + B
 \end{tikzcd}
\]
In Haskell, the type $A + B$ is called \hask{Either A B}. By analogy with \hask{Bool}, we can define it as
\begin{haskell}
data Either A B where
  Left  :: A -> Either A B
  Right :: B -> Either A B
\end{haskell}

By analogy, the mapping out from $A + B$ to some type $C$ is determined by this diagram:
\[
 \begin{tikzcd}
 A
 \arrow[dr,  bend left, "\text{Left}"']
 \arrow[ddr, bend right, "f"']
 && B
 \arrow[dl, bend right, "\text{Right}"]
 \arrow[ddl, bend left, "g"]
 \\
&A + B
\arrow[d, dashed, "h"]
\\
& C
 \end{tikzcd}
\]
Given a function $h$, we get a pair of functions $f$ and $g$ just by composing it with \hask{Left} and \hask{Right}. Conversely, such a pair of functions uniquely determines $h$. 

When we want to translate this diagram to Haskell, we need to select elements of the two types. We can do it by defining the arrows $a$ and $b$ from the terminal object. 
\[
 \begin{tikzcd}
 &1
 \arrow[ld, "a"']
 \arrow[rd, "b"]
 \\
 A
 \arrow[dr,  bend left, "\text{Left}"']
 \arrow[ddr, bend right, "f"']
 && B
 \arrow[dl, bend right, "\text{Right}"]
 \arrow[ddl, bend left, "g"]
 \\
&A + B
\arrow[d, dashed, "h"]
\\
& C
 \end{tikzcd}
\]
Follow the arrows in this diagram to get
\[h \circ \text{Left} \circ a = f \circ a\]
\[h \circ \text{Right} \circ b = g \circ b\]

Haskell syntax repeats these equations almost literally, resulting in this pattern-matching syntax for the definition of \hask{h}:

\begin{haskell}
h :: Either A B -> C
h (Left  a) = f a
h (Right b) = g b
\end{haskell}

Read these equations right to left and you will see the computation rules for sum types. The two functions that were used to define \hask{h} can be recovered by applying \hask{h} to terms constructed using \hask{Left} and \hask{Right}. 

You can also use the \hask{case} syntax to define \hask{h}:
\begin{haskell}
h e = case e of
  Left  a -> f a
  Right b -> g b
\end{haskell}

In logic, the proposition $A + B$ is called the alternative, or \emph{logical or}. You can prove it by providing the proof of $A$ or the proof of $B$. Either one will suffice. 

If you want to prove that $C$ follows from $A+B$, you have to be prepared for two eventualities: either the proof of $A+B$ was obtained by proving $A$ or by proving $B$. In the first case, you need a proof that $C$ follows from $A$. In the second case you need a proof that $C$ follows from $B$. These are the arrows in the definition of $A+B$.

\section{Properties of Sums}

We speak of equality of arrows, but we speak of isomorphism of objects, which in turn is defined by equality of arrows.

When we speak of numbers, we understand that $1 + 0 = 1$. When we speak of objects,  all we can say is that $1 + 0 \cong 1$. 

The easy way to show that two objects are isomorphic is to consider how they interact with other objects. For sums, you look at the outgoing arrows: the mappings out. 

Look at the definition of $1 + 0$ and it's mapping out to any object $A$. It's defined by a pair $(\hask{a}, \hask{absurd})$, where $a$ is an element of $A$. 

\[
 \begin{tikzcd}
 1
 \arrow[dr,  bend left, "\text{Left}"']
 \arrow[ddr, bend right, "a"']
 && 0
 \arrow[dl, bend right, "\text{Right}"]
 \arrow[ddl, bend left, "\text{absurd}"]
 \\
&1 + 0
\arrow[d, dashed, "h"]
\\
& A
 \end{tikzcd}
\]
There is only one \hask{absurd}, but there can be many elements of \hask{A}, and each \hask{a} defines a different \hask{h}. We see that the two objects $1$ and $1 + 0$ have the same set of outgoing arrows to any object $A$. 

\chapter{Product Types}

Digits vs decimal numbers '1' '0'
\section{notes}

logical interpretation  of + 

$0 + a \cong a$

$a + b \cong b + a$
 
$(a + b) + c \cong a + (b + c)$

\begin{haskell}
\end{haskell}


\end{document}